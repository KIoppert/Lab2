{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### <center></center>\n",
        "# <center><b>Отчёт по лабораторной работе №1</b></center>\n",
        "Выполнили:\n",
        "\n",
        "+ Горобец Иван M3236\n",
        "\n",
        "+ Морозов Андрей M3236\n",
        "\n",
        "+ Цвей Лев M3236\n",
        "\n",
        "+ Илюхин Артем M3236\n",
        "\n",
        "### <center>Постановка задачи</center>\n",
        "Исходная задача представляет собой поиск точки минимума функции $f: O \\subset \\mathbb{R}^n → \\mathbb{R}, \\space f ∈ C^1(O)$. В нашей работе ограничимся случаями n = 1, 2. Общая стратегия отыскания точки экстремума будет следующей: задаём начальную точку $x_0$ далее строим последовательность точек $x_k$ следующим образом: $x_{k+1} = x_k - \\nabla f(x_k) \\cdot learning \\_  rate$ - то есть каждый раз двигаемся по направлению антиградиента. Основной трудностью является оптимальный выбор $learning\\_rate$ на каждом шаге.\n",
        "\n",
        "### <center>Критерий остановки</center>\n",
        "Также необходимо понять, в каких случаях нужно остановить вычисления. Так как мы ищем точку минимума $f(x)$, то для нас таковым будет условие $|∇ f(x)| < ε$, где $ε$ - достаточно маленькая числовая константа. Также поставим отсечку по количеству итераций на $10^6$ шагов.\n",
        "\n",
        "### <center>Реализованные стратегии выбора шага</center>\n",
        "+ `Constant`: $learning\\_rate$ не изменяется и на каждом шаге равен изначально заданной константе.\n",
        "+ `Polynomial decay`: $learning\\_rate(k) = \\displaystyle \\frac 1 {\\sqrt k}$\n",
        "+ `Кусочно-постоянный`: $learning\\_rate(k) = \\displaystyle \\frac 1 {2^m}, \\space \\frac 1 {2^m} \\le k \\le \\frac 1 {2^{m-1}}$\n",
        "+ `Экспоненциальный`: $learning\\_rate(k) = \\displaystyle \\frac 1 {e^2} e^{-t k}$\n",
        "\n",
        "## <center>Методы одномерного спуска</center>\n",
        "\n",
        "+ правило Арамихо\n",
        "+ правило Вольфе-Пауэлла\n",
        "+ Дихотомия\n",
        "+ Метод золотого сечения\n",
        "\n",
        "_Каждый метод подробно описан в соответствующей ячейке_"
      ],
      "metadata": {
        "id": "36lES0clUjF3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLbxlnOvCM6Y"
      },
      "outputs": [],
      "source": [
        "from copy import copy, deepcopy\n",
        "from math import sqrt, sin, pow, log\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import typing as tp\n",
        "from collections.abc import Callable\n",
        "import random\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
        "from scipy.optimize import minimize\n",
        "import numdifftools as nd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "Блоки кода, отвечающие за необходимые импорты библиотек для корректной работы программы"
      ],
      "metadata": {
        "id": "1re3xG7mo06v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVocsnqgR9c7"
      },
      "outputs": [],
      "source": [
        "# Вынесенные общие типы для функций\n",
        "\n",
        "# Определение типа функции одной переменной\n",
        "Func1D = Callable[[float], float]\n",
        "# Определение типа функции двух переменных\n",
        "Func2D = Callable[[np.ndarray], float]\n",
        "# Определение типа планировщика обучения\n",
        "Scheduler = Callable[[Func2D, np.ndarray, int], float]\n",
        "# Определение типа правила поиска\n",
        "SearchRule = Callable[[Func1D, float, float], float]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSWgO9DNp3n3"
      },
      "outputs": [],
      "source": [
        "def grad_desc_plot_2d(\n",
        "        func: Func2D,\n",
        "        steps: np.ndarray,\n",
        "        ax: plt.Axes,\n",
        "        xlim: tuple[float, float],\n",
        "        ylim: tuple[float, float],\n",
        "        cmap: str = \"viridis\",\n",
        "        title: str = \"\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Функция отрисовки шагов градиентного спуска для функции 2х переменных.\n",
        "\n",
        "    :param func: функция, которая минимизируется градиентным спуском\n",
        "    :param steps: np.array[N x 2] — шаги алгоритма\n",
        "    :param ax: холст для отрисовки графика\n",
        "    :param xlim: tuple(float), 2 — диапазон по первой оси\n",
        "    :param ylim: tuple(float), 2 — диапазон по второй оси\n",
        "    :param cmap: str — название палитры\n",
        "    :param title: str — заголовок графика\n",
        "    \"\"\"\n",
        "\n",
        "    ax.set_title(title, fontsize=40, fontweight=\"bold\")\n",
        "\n",
        "    x_range = np.linspace(*xlim, 100)\n",
        "    y_range = np.linspace(*ylim, 100)\n",
        "    grid = np.meshgrid(x_range, y_range)\n",
        "    X, Y = grid\n",
        "    fvalues = func(\n",
        "        np.dstack(grid).reshape(-1, 2)\n",
        "    ).reshape((x_range.size, y_range.size))\n",
        "    ax.pcolormesh(x_range, y_range, fvalues, cmap=cmap, alpha=0.8)\n",
        "    CS = ax.contour(x_range, y_range, fvalues)\n",
        "    ax.clabel(CS, CS.levels, inline=True)\n",
        "\n",
        "    arrow_kwargs = dict(linestyle=\"--\", color=\"black\", alpha=0.8)\n",
        "    for i, _ in enumerate(steps):\n",
        "        if i + 1 < len(steps):\n",
        "            ax.arrow(\n",
        "                *steps[i],\n",
        "                *(steps[i+1] - steps[i]),\n",
        "                **arrow_kwargs\n",
        "            )\n",
        "\n",
        "    n = len(steps)\n",
        "    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n",
        "    ax.scatter(steps[:, 0], steps[:, 1], c=color_list, zorder=10)\n",
        "    ax.scatter(steps[-1, 0], steps[-1, 1],\n",
        "               color=\"red\", label=f\"estimate = {np.round(steps[-1], 2)}\")\n",
        "\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.set_ylabel(\"$y$\")\n",
        "    ax.set_xlabel(\"$x$\")\n",
        "    ax.legend(fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок кода, содержащий функцию вычисления градиента"
      ],
      "metadata": {
        "id": "pMdR2YvVpKAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqEyuBGmZQVx"
      },
      "outputs": [],
      "source": [
        "def numeric_gradient(f: Func2D, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Функция для высчитывания значения градиента функции f в точке x с использованием центральных разностей.\n",
        "\n",
        "    :param f: Анализируемая функция\n",
        "    :type f: tp.Callable\n",
        "    :param x: Точка\n",
        "    :type x: np.array\n",
        "    \"\"\"\n",
        "    eps = 2 ** (-52) # Machine epsilon for 64-bit doubles\n",
        "\n",
        "    h_x = sqrt(eps) * x[0] if x[0] != 0 else sqrt(eps)\n",
        "    h_y =  sqrt(eps) * x[1] if x[1] != 0 else sqrt(eps)\n",
        "\n",
        "    approx_x = (f(x + np.array([h_x, 0])) -\n",
        "              f(x - np.array([h_x, 0]))) / (2 * h_x)\n",
        "\n",
        "    approx_y = (f(x + np.array([0, h_y])) -\n",
        "              f(x - np.array([0, h_y]))) / (2 * h_y)\n",
        "\n",
        "    return np.array([approx_x, approx_y])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Константная стратегия выбора шага. На каждой итерации алгоритма градиетного спуска всегда возвращает постоянное значение."
      ],
      "metadata": {
        "id": "N92NqhfppPJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVMWBVB2hmNh"
      },
      "outputs": [],
      "source": [
        "def constant_scheduler(f: Func2D, x: np.ndarray, previous_lr: float = 0.01) -> float:\n",
        "  \"\"\"\n",
        "  Возвращает постоянное значение learning rate (lr).\n",
        "\n",
        "  :param f: Целевая функция\n",
        "  :param x: Текущая точка\n",
        "  :param previous_lr: Предыдущее значение lr\n",
        "  :return: 0.01\n",
        "  \"\"\"\n",
        "  return 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Экспоненциальная** стратегия выбора шага. Чем больше итераций алгоритма градиентного спуска было выполнено, тем сильнее затухает `learning_rate`."
      ],
      "metadata": {
        "id": "EBGYlV2Hpk5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "230i6XhmKjrs"
      },
      "outputs": [],
      "source": [
        "def exp_scheduler(start: float = 3e-2, t: float = 0.001) -> Scheduler:\n",
        "  \"\"\"\n",
        "  Фабрика для создания экспоненциального планировщика learning rate.\n",
        "  Возвращает функцию, которая вычисляет learning rate по формуле:\n",
        "  lr(n) = start * exp(-t * n), где n - номер итерации.\n",
        "\n",
        "  :param start: Начальное значение learning rate (по умолчанию 3e-2).\n",
        "  :param t: Коэффициент скорости затухания (по умолчанию 0.001).\n",
        "  :return: Функия, возвращающая значение learning rate для итерации n.\n",
        "  \"\"\"\n",
        "  def wrapper_scheduler(f, x, n):\n",
        "    return start * np.exp(-t * n)\n",
        "  return wrapper_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Полиномиальная** стратегия выбора шага. Данный метод позволяет сохранить высокую скорость спуска на начальном этапе и уменьшать `learning_rate` по мере приближения к ответу.\n",
        "\n",
        "> Добавить блок с цитатой\n",
        "\n"
      ],
      "metadata": {
        "id": "_8p9c_sJ_x-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polynom_scheduler(\n",
        "        steps: int = 10000,\n",
        "        init_lr: float = 0.05,\n",
        "        end_lr: float = 0.0001,\n",
        "        power: float = 0.5\n",
        ") -> Scheduler:\n",
        "    \"\"\"\n",
        "    Фабрика для создания полиномиального планировщика learning rate.\n",
        "    Возвращает функцию, которая вычисляет learning rate.\n",
        "\n",
        "    :param steps: Количество шагов между init_lr и end_lr (по умолчанию 10_000).\n",
        "    :param init_lr: Стартовый learning rate (по умолчанию 0.01).\n",
        "    :param end_lr: Минимальный learning rate (по умолчанию 0.0001).\n",
        "    :param power: Степень полинома (по умолчанию 1).\n",
        "    :return: Функия, возвращающая значение learning rate для итерации n.\n",
        "    \"\"\"\n",
        "\n",
        "    def wrapper_scheduler(f, x, n):\n",
        "      n = min(n, steps)\n",
        "      return (init_lr - end_lr) * (1 - n / steps) ** (power) + end_lr\n",
        "\n",
        "    return wrapper_scheduler"
      ],
      "metadata": {
        "id": "d3gF3zF_AXUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Кусочно-постоянная** стратегия выбора шага"
      ],
      "metadata": {
        "id": "Nr-L3aGBu4fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_const_scheduler(\n",
        "    init_lr: float = 0.1,\n",
        "    end_lr: float = 0.001,\n",
        "    power: float = 2\n",
        ") -> Scheduler:\n",
        "    \"\"\"\n",
        "    Фабрика для создания кусочно-постоянного планировщика learning rate.\n",
        "    Возвращает функцию, которая вычисляет learning rate.\n",
        "\n",
        "    :param init_lr: Стартовый learning rate (по умолчанию 0.01).\n",
        "    :param end_lr: Минимальный learning rate (по умолчанию 0.0001).\n",
        "    :param power: Понижающий коэффициент (по умолчанию 2).\n",
        "    :return: Функия, возвращающая значение learning rate для итерации n.\n",
        "    \"\"\"\n",
        "\n",
        "    def wrapper_scheduler(f, x, n):\n",
        "      n = int(log(n + 1, power))\n",
        "      return max(init_lr * (1 / (power ** n)), end_lr)\n",
        "\n",
        "    return wrapper_scheduler"
      ],
      "metadata": {
        "id": "bCN0G-vKu47s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center></center>\n",
        "### **<center>Правило Арамихо</center>**\n",
        "\n",
        "$$ f(x_k - \\lambda \\nabla f(x_k)) < f(x_k) - \\sigma \\lambda \\space||\\nabla f(x_k)||^2 \\space $$\n",
        "\\- это условие Арамихо. Вычисление $learning\\_rate$ не будет зависеть от номера текущего шага, а будет определяться следующим образом:\n",
        "\n",
        "Изначально $\\lambda = \\lambda_0$, пока условие Арамихо не выполняется: $\\lambda_{new} = \\beta \\cdot \\lambda_{prev}$. Константы $\\lambda_0, \\space \\beta, \\space \\sigma$, являются `hyper`параметрами, заданы изначально и не меняются в ходе работы."
      ],
      "metadata": {
        "id": "WrOlPjVqtqtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaY85feSelc0"
      },
      "outputs": [],
      "source": [
        "def armijo_rule(*, sigma: float = 0.3, beta: float = 0.8) -> Scheduler:\n",
        "  \"\"\"\n",
        "  Фабрика для создания правила Армихо для выбора learning rate.\n",
        "\n",
        "  :param sigma: Гипер параметр: условия достаточного убывания (по умолчанию 0.3).\n",
        "  :param beta: Гипер параметр: Коэффициент уменьшения шага (по умолчанию 0.8).\n",
        "  :return: Функция, принимающая (f, x, previous_lr) и возвращающая новый lr.\n",
        "  \"\"\"\n",
        "  def wrapper(f, x, previous_lr = 1):\n",
        "    lr = previous_lr\n",
        "    p_k = -numeric_gradient(f, x)\n",
        "\n",
        "    rule = lambda a: f(x + p_k * a) <= f(x) - \\\n",
        "                      sigma * lr * np.linalg.norm(-p_k) ** 2\n",
        "\n",
        "    while not rule(lr):\n",
        "      lr *= beta\n",
        "\n",
        "    return lr\n",
        "\n",
        "  return wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center></center>\n",
        "### **<center>Правило Вольфе-Пауэлла</center>**\n",
        "Два условия:\n",
        " $$f(x_k - \\lambda \\nabla f(x_k)) \\le f(x_k) - C_1 \\lambda \\space||\\nabla f(x_k)||^2 \\quad \\quad (1)$$\n",
        " $$-\\nabla f(x_k) \\cdot \\nabla f(x_k - \\lambda \\nabla f(x_k))  \\le C_2 \\space||\\nabla f(x_k)||^2 \\quad (2)$$\n",
        " как и в предыдущем правиле, пока не выполняется хотя бы одно из условий, меняем шаг: $\\lambda_{new} = \\beta \\cdot \\lambda_{prev}$.\n",
        "\n",
        " Константы $C_1, C_2, \\beta \\space -$ являются `hyper`параметрами, заданы изначально и не меняются в ходе работы."
      ],
      "metadata": {
        "id": "cq_pGjGMupGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osFi_rameo8l"
      },
      "outputs": [],
      "source": [
        "def wolfe_rule(*, C1: float = 1e-4, C2: float = 0.9, beta: float = 0.8) -> Scheduler:\n",
        "  \"\"\"\n",
        "  Фабрика для создания правила Вольфе для выбора learning rate.\n",
        "\n",
        "  :param C1: Гипер параметр: Параметр условия достаточного убывания (по умолчанию 1e-4).\n",
        "  :param C2: Гипер параметр: Параметр условия кривизны (по умолчанию 0.9).\n",
        "  :param beta: Гипер параметр: Коэффициент уменьшения шага (по умолчанию 0.8).\n",
        "  :return: Функция, принимающая (f, x, previous_lr) и возвращающая новый lr.\n",
        "  \"\"\"\n",
        "  def wrapper(f, x, previous_lr = 1):\n",
        "\n",
        "    lr = previous_lr\n",
        "    p_k = -numeric_gradient(f, x)\n",
        "\n",
        "    rule_I = lambda a: f(x + p_k * a) <= f(x) - C1 * a * np.linalg.norm(p_k) ** 2\n",
        "    rule_II = lambda a: np.dot(numeric_gradient(f, x + a * p_k), -p_k) \\\n",
        "                          >= -C2 * np.linalg.norm(p_k) ** 2\n",
        "\n",
        "    while not rule_I(lr) or not rule_II(lr):\n",
        "      lr *= beta\n",
        "    return lr\n",
        "\n",
        "  return wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-e6KkdGymza"
      },
      "outputs": [],
      "source": [
        "# def goldstein_rule(*, dzeta_1 = 0.01, dzeta_2 = 0.85, beta = 0.5):\n",
        "#   \"\"\"\n",
        "#   Фабрика для создания правила Гольдштейна для выбора learning rate.\n",
        "\n",
        "#   :param dzeta_1: Гипер параметр: Верхний параметр условия (по умолчанию 0.01).\n",
        "#   :param dzeta_2: Гипер параметр: Нижний параметр условия (по умолчанию 0.85).\n",
        "#   :param beta: Гипер параметр: Коэффициент уменьшения шага (по умолчанию 0.5).\n",
        "#   :return: Функция, принимающая (f, x, previous_lr) и возвращающая новый lr.\n",
        "#   \"\"\"\n",
        "#   def wrapper(f, x, previous_lr = 1):\n",
        "\n",
        "#     lr = previous_lr\n",
        "#     p_k = -numeric_gradient(f, x)\n",
        "\n",
        "#     rule = lambda a: dzeta_2 * (a * np.dot(p_k, -p_k)) <= \\\n",
        "#                      f(x + a * p_k) - f(x) <= \\\n",
        "#                      dzeta_1 * (a * np.dot(p_k, -p_k))\n",
        "\n",
        "#     while not rule(lr):\n",
        "#         lr *= beta\n",
        "#     return lr\n",
        "\n",
        "#   return wrapper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center></center>\n",
        "### **<center>Метод дихотомии</center>**\n",
        "Работа этого метода сильно опирается на унимодальность функции. \\\\\n",
        "У нас есть отрезок, на котором мы ищем минимум. Для этого мы делим его каждый раз пополам, а потом сравниваем значение в средней точке с значениями в серединах получившихся маленьких отрезков. За счет унимодальности мы можем гарантированно сказать, в какой части изначального отрезка находится минимум, после чего сдвинуть его границы на новый отрезок \\\\\n",
        "<big><strong>Плюсы данного алгоритма:</strong></big>\n",
        "<ol>\n",
        "<li>Простота реализации (всего 10 строк)</li>\n",
        "<li>Он работает за меньшее количество шагов чем метод золотого сечения</li>\n",
        "</ol>\n",
        "<big><strong>Минусы:</strong></big>\n",
        "<ol>\n",
        "<li>Необходимость много раз считать значени функции в точке (если это долго, то могут быть проблемы с эфективностью алгоритма)</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "U4fofnkp30t9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byg0u1UvVu1J"
      },
      "outputs": [],
      "source": [
        "def dihotomy_method_unimodal(f: Func1D, start: float, end: float, tol: float = 1e-6) -> float:\n",
        "  \"\"\"\n",
        "  Поиск минимума унимодальной функции на отрезке при помощи метода дихотомии (половинным делением)\n",
        "\n",
        "  :param f: унимодальная функция\n",
        "  :param start: начало отрезка, на котором ищем минимум\n",
        "  :param end: конец отрезка, на котором ищем минимум\n",
        "  :param tol: точность (наибольшая разность между точкой в которой достигается минимум и возвращаемым значением)\n",
        "  \"\"\"\n",
        "\n",
        "  while end - start > tol:\n",
        "    mid = (end + start) / 2\n",
        "    if f(mid) > f((start + mid) / 2):\n",
        "      end = mid\n",
        "    elif f(mid) > f((mid + end) / 2):\n",
        "      start = mid\n",
        "    else:\n",
        "      start = (start + mid) / 2\n",
        "      end = (end + mid) / 2\n",
        "\n",
        "  return (start + end) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center></center>\n",
        "### **<center>Метод золотого сечения</center>**\n",
        "Этот метод, как и предыдущий опирается на унимодальность функции, а также работает при помощи сужения отрезка в константное число раз на каждой итерации\n",
        "\n",
        "<big><strong>Но есть два важных отличия:</strong><big>\n",
        "<ol>\n",
        "<li>В методе дихотомии длина отрезка каждый раз сокращалась в $2$ раза, а в этом методе в $\\phi = \\frac{1 + \\sqrt{5}}{2} \\approx 1.618$</li>\n",
        "<li>Однако за счет этого делается меньше вычислений значения функции в точке!</li>\n",
        "</ol>\n",
        "\n",
        "За счет чего достигается второй пункт?\n",
        "\n",
        "На кадом этапе, вместо того чтобы бить отрезок на 4 равные части и считать значения в каждой из 3 средних точке, мы будем бить его в отношении $1: \\phi$, а также $1:(\\phi-1)$. Таким образом мы получим две точки, сравнив значения в которых и значения на концах мы сможем понять на каком из отрезков находиться минимум. Однако! В отличие от прошлого метода, когда мы выберем этот отрезок, оба конца и одна из средних точек уже будет подсчитана, за счет того что длины отрезков относятся как золотое сечение. Это позволяет на каждой итерации делать ровно одно вычисление значения функции в точке, что может ускорить код, если это дорогостоящая операция"
      ],
      "metadata": {
        "id": "ZRwWtUhI56Lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPd7IaumbGin"
      },
      "outputs": [],
      "source": [
        "def golden_ratio_method_unimodal(f: Func1D, start: float, end: float, tol: float = 1e-6) -> float:\n",
        "    \"\"\"\n",
        "    Поиск минимума унимодальной функции на отрезке при помощи метода золотого сечения\n",
        "\n",
        "    :param f: унимодальная функция\n",
        "    :param start: начало отрезка, на котором ищем минимум\n",
        "    :param end: конец отрезка, на котором ищем минимум\n",
        "    :param tol: точность (наибольшая разность между точкой в которой достигается минимум и возвращаемым значением)\n",
        "    \"\"\"\n",
        "\n",
        "    # константа золотого сечения\n",
        "    phi: float = (1 + sqrt(5)) / 2\n",
        "    # части на которые делится отрезок\n",
        "    bigger_part: float = phi - 1\n",
        "    smaller_part: float = 1 - bigger_part\n",
        "    c = start + smaller_part * (end - start)\n",
        "    d = start + bigger_part * (end - start)\n",
        "\n",
        "    if (val1 := f(c)) > (val2 := f(d)):\n",
        "        start = c\n",
        "        # сохраняем результат для неиспользованной точки чтобы не пересчитывать его в будущем\n",
        "        saved_dot: float = d\n",
        "        saved_value: float = val2\n",
        "        chose_left_on_last_choice: bool = True\n",
        "    else:\n",
        "        end = d\n",
        "        saved_dot: float = c\n",
        "        saved_value: float = val1\n",
        "        chose_left_on_last_choice: bool = False\n",
        "\n",
        "    while end - start > tol:\n",
        "        new_dot = start + (end - start) * (bigger_part if chose_left_on_last_choice else smaller_part)\n",
        "        if (new_value := f(new_dot)) < saved_value:\n",
        "            if chose_left_on_last_choice:\n",
        "                start = saved_dot\n",
        "            else:\n",
        "                end = saved_dot\n",
        "            saved_value = new_value\n",
        "            saved_dot = new_dot\n",
        "        else:\n",
        "            if chose_left_on_last_choice:\n",
        "                end = new_dot\n",
        "            else:\n",
        "                start = new_dot\n",
        "            chose_left_on_last_choice ^= True\n",
        "\n",
        "    return (start + end) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок кода, с реализацией алгоритма градиентого спуска."
      ],
      "metadata": {
        "id": "VrwhWW5AqD-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwWBZJ1HyW6u"
      },
      "outputs": [],
      "source": [
        "def grad_desc_with_rule(\n",
        "        func: Func2D,\n",
        "        start: np.ndarray,\n",
        "        lr: Callable[..., float],\n",
        "        max_iter: int = 10**6,\n",
        "        eps: float = 1e-6\n",
        ") -> tuple[np.ndarray, list[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Finding function minimum using gradient descent.\n",
        "\n",
        "    :param func: function to be analyzed\n",
        "    :type func: Callable[np.array(x, y)]\n",
        "    :param start: starting point\n",
        "    :type start: np.array(x, y)\n",
        "    :param lr: rule for learning rate calculation\n",
        "    :type lr: Callable(func, np.array(x, y), float)\n",
        "    :param max_iter: maximum amount of iterations\n",
        "    :type max_iter: int\n",
        "    :param eps: stop criteria\n",
        "    :type eps: float\n",
        "\n",
        "    :returns: minimum value and history of values\n",
        "    \"\"\"\n",
        "\n",
        "    x = start\n",
        "    history = [x]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        if \"scheduler\" in lr.__name__:\n",
        "            learning_rate = lr(func, x, i)\n",
        "        else:\n",
        "            learning_rate = lr(func, x)\n",
        "        gradient = numeric_gradient(func, x)\n",
        "        x_new = x - learning_rate * gradient\n",
        "        if np.linalg.norm(gradient) < eps:\n",
        "            break\n",
        "        x = x_new\n",
        "        history.append(x)\n",
        "\n",
        "    return x, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQj9EfN2OqUS"
      },
      "outputs": [],
      "source": [
        "def grad_desc_on_1dsearch(\n",
        "        func: Func2D,\n",
        "        start: np.ndarray,\n",
        "        search_rule: SearchRule,\n",
        "        max_iter: int = 10**8,\n",
        "        eps: float = 1e-6,\n",
        "        lr_min: float = 0,\n",
        "        lr_max: float = 1\n",
        ") -> tuple[np.ndarray, list[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Finding function minimum using gradient descent.\n",
        "    Learning rate is calculated used a specified 1d_search algorithm\n",
        "\n",
        "    :param func: function to be analyzed\n",
        "    :type func: Callable[np.array(x, y)]\n",
        "    :param start: starting point\n",
        "    :type start: np.array(x, y)\n",
        "    :param search_rule: rule for learning rate calculation\n",
        "    :type lr: Callable(func, np.array(x, y), float)\n",
        "    :param max_iter: maximum amount of iterations\n",
        "    :type max_iter: int\n",
        "    :param eps: stop criteria\n",
        "    :type eps: float\n",
        "\n",
        "    :returns: minimum value and history of values\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    x = start\n",
        "    history = [x]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        gradient = numeric_gradient(func, x)\n",
        "        func_to_minimize = lambda lr: func(x - gradient * lr)\n",
        "        learning_rate = search_rule(func_to_minimize, lr_min, lr_max)\n",
        "        x_new = x - learning_rate * gradient\n",
        "        if np.linalg.norm(gradient) < eps:\n",
        "            break\n",
        "        x = x_new\n",
        "        history.append(x)\n",
        "\n",
        "    return x, history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок кода, генерирующий на основе заданной функции функцию с шумом."
      ],
      "metadata": {
        "id": "GLSoSYWY2K6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func_with_noise(func: Func2D, noise_coeff: float = 2) -> Func2D:\n",
        "  def wrapper_func(x):\n",
        "    return func(x) + np.random.random(2) * noise_coeff"
      ],
      "metadata": {
        "id": "fqxdn7Ty2hWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основной блок кода, показывающий принцип работы алгоритмов на различных функциях и визуализирующий сам процесс."
      ],
      "metadata": {
        "id": "r-M1vRriqROF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_functions = {\n",
        "    \"Quadratic_1\": lambda x: x[0] * x[0] + x[1] * x[1] - 6 * x[0] - 8 * x[1] + 10,\n",
        "    \"Quadratic_2\": lambda x: 2 * x[0] * x[0] + 3 * x[1] * x[1] - 12 * x[0] - 18 * x[1] + 20,\n",
        "    \"Quadratic_3\": lambda x: 4 * x[0] * x[0] + 5 * x[1] * x[1] + 2 * x[0] * x[1] - 16 * x[0] - 20 * x[1] + 15,\n",
        "\n",
        "    \"Multimodal_trigonom\": lambda x: sin(0.5 * x[0]) * sin(0.5 * x[0]) + sin(0.5 * x[1]) * sin(0.5 * x[1]),\n",
        "    \"Hard function_1\": lambda x: - 1 / (x[0] * x[0] + x[1] * x[1] + 1) - 1 / pow(\n",
        "        (x[0] - 2) * (x[0] - 2) + (x[1] - 2) * (x[1] - 2) + 1, 2),\n",
        "\n",
        "    \"Himmelblau\": lambda x: pow(x[0] * x[0] + x[1] - 11, 2) + pow(x[0] + x[1] * x[1] - 7, 2),\n",
        "}\n",
        "\n",
        "rules = {\n",
        "    \"Constant_scheduler\": constant_scheduler,\n",
        "    \"Exp_scheduler\": exp_scheduler(),\n",
        "    \"Polynom_scheduler\": polynom_scheduler(),\n",
        "    \"Partial_const_scheduler\": partial_const_scheduler(),\n",
        "    \"Armijo_rule\": armijo_rule(),\n",
        "    \"Wolfe_rule\": wolfe_rule()\n",
        "}\n",
        "\n",
        "search_1d = {\n",
        "    \"Dihotomy\": dihotomy_method_unimodal,\n",
        "    \"Golden_ratio\": golden_ratio_method_unimodal\n",
        "}"
      ],
      "metadata": {
        "id": "F-Zi3tSHqe_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS9hGfYS1ANb"
      },
      "outputs": [],
      "source": [
        "# for func_name, func in test_functions.items():\n",
        "#     point = np.random.randn(2)\n",
        "\n",
        "#     fig = plt.figure(figsize=(50, 40), constrained_layout=True)\n",
        "\n",
        "#     gs = GridSpec(3, 1, height_ratios=[0.1, 0.8, 1.4], figure=fig)\n",
        "\n",
        "#     title_ax = fig.add_subplot(gs[0])\n",
        "#     title_ax.axis('off')\n",
        "#     title_ax.set_title(func_name, fontsize=55, fontweight=\"bold\")\n",
        "\n",
        "#     gs_3d = GridSpecFromSubplotSpec(1, 3, subplot_spec=gs[1], wspace=0)\n",
        "#     ax_3d = fig.add_subplot(gs_3d[0, 1], projection='3d')\n",
        "\n",
        "#     x = np.linspace(-5, 5, 100)\n",
        "#     y = np.linspace(-5, 5, 100)\n",
        "#     X, Y = np.meshgrid(x, y)\n",
        "#     Z = np.array([func([xi, yi]) for xi, yi in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
        "\n",
        "#     surf = ax_3d.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.8, linewidth=0, antialiased=True)\n",
        "\n",
        "#     ax_3d.set_title(\"3D Visualization\", fontsize=40)\n",
        "#     ax_3d.set_xlabel('X', fontsize=12, labelpad=10)\n",
        "#     ax_3d.set_ylabel('Y', fontsize=12, labelpad=10)\n",
        "#     ax_3d.set_zlabel('Z', fontsize=12, labelpad=10)\n",
        "#     ax_3d.view_init(elev=30, azim=45)\n",
        "\n",
        "#     cbar = fig.colorbar(surf, ax=ax_3d, shrink=0.6, aspect=20, pad=0.05)\n",
        "#     cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "#     inner_gs = GridSpecFromSubplotSpec(3, 3, subplot_spec=gs[2], wspace=0.01, hspace=0.01)\n",
        "\n",
        "#     ax_2d = []\n",
        "#     for i in range(3):\n",
        "#         for j in range(3):\n",
        "#             ax_2d.append(fig.add_subplot(inner_gs[i, j]))\n",
        "#             ax_2d[-1].set_aspect('equal', adjustable='box')\n",
        "\n",
        "#     i = 0\n",
        "\n",
        "#     for rule_name, rule in rules.items():\n",
        "#         try:\n",
        "#           res, search_log = grad_desc_with_rule(func, point, rule)\n",
        "#           search_points = np.vstack(search_log)\n",
        "#         except Exception as e:\n",
        "#           print(rule_name)\n",
        "#           print(e)\n",
        "#           print(\"-------------------------------------------------\")\n",
        "#         grad_desc_plot_2d(\n",
        "#             np.vectorize(func, signature=\"(n)->()\"),\n",
        "#             search_points,\n",
        "#             ax=ax_2d[i],\n",
        "#             xlim=(-5, 5),\n",
        "#             ylim=(-5, 5),\n",
        "#             title=rule_name\n",
        "#         )\n",
        "\n",
        "#         i += 1\n",
        "\n",
        "#     for search_name, search in search_1d.items():\n",
        "#         res, search_log = grad_desc_on_1dsearch(func, point, search)\n",
        "#         search_points = np.vstack(search_log)\n",
        "\n",
        "#         grad_desc_plot_2d(\n",
        "#             np.vectorize(func, signature=\"(n)->()\"),\n",
        "#             search_points,\n",
        "#             ax=ax_2d[i],\n",
        "#             xlim=(-5, 5),\n",
        "#             ylim=(-5, 5),\n",
        "#             title=search_name\n",
        "#         )\n",
        "#         i += 1\n",
        "\n",
        "#     for ax in ax_2d:\n",
        "#         ax.grid(True, linestyle='--', alpha=0.7)\n",
        "#         ax.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "#     plt.show()\n",
        "#     print(\"_________________________________________________\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из библиотеки `SciPy` были использованы методы `BFGS` (алгоритм Бройдена–Флетчера–Гольдфарба–Шанно) и `CG` (метод сопряжённых градиентов)\n",
        "\n"
      ],
      "metadata": {
        "id": "7PEmkPSVDFQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_direction(f, x):\n",
        "    grad = numeric_gradient(f, x)\n",
        "    try:\n",
        "        return np.dot(-np.linalg.inv(nd.Hessian(f)(x)), grad)\n",
        "    except np.linalg.LinAlgError:\n",
        "        return -grad"
      ],
      "metadata": {
        "id": "us7SVk9tL8Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f = list(test_functions.items())[0]\n",
        "# print(nd.Hessian(f[1])(np.array([1.0, 2.0])))\n",
        "\n",
        "def newton_cg(\n",
        "        func: Func2D,\n",
        "        start: np.ndarray,\n",
        "        sch: Callable[..., float],\n",
        "        max_iter: int = 10**6,\n",
        "        eps: float = 1e-6\n",
        ") -> tuple[np.ndarray, list[np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Finding function minimum using gradient descent.\n",
        "\n",
        "    :param func: function to be analyzed\n",
        "    :type func: Callable[np.array(x, y)]\n",
        "    :param start: starting point\n",
        "    :type start: np.array(x, y)\n",
        "    :param lr: rule for learning rate calculation\n",
        "    :type lr: Callable(func, np.array(x, y), float)\n",
        "    :param max_iter: maximum amount of iterations\n",
        "    :type max_iter: int\n",
        "    :param eps: stop criteria\n",
        "    :type eps: float\n",
        "\n",
        "    :returns: minimum value and history of values\n",
        "    \"\"\"\n",
        "\n",
        "    x = start\n",
        "    history = [x]\n",
        "    for i in range(max_iter):\n",
        "        if \"scheduler\" in sch.__name__:\n",
        "            learning_rate = sch(func, x, i) * 100\n",
        "        else:\n",
        "            learning_rate = sch(func, x)\n",
        "\n",
        "        direction = compute_direction(func, x)\n",
        "        x_new = x + learning_rate * direction\n",
        "        if np.linalg.norm(numeric_gradient(func, x)) < eps:\n",
        "            break\n",
        "        # print(np.linalg.norm(numeric_gradient(func, x)))\n",
        "        x = x_new\n",
        "        history.append(x)\n",
        "\n",
        "    return x, history\n",
        "\n",
        "# from functools import partial\n",
        "# initial_point = (-0.0031, 0.206)\n",
        "# min_ = (3, 2)\n",
        "# for name, f in [list(test_functions.items())[5]]:\n",
        "#   for method in [\"BFGS\", \"CG\"]:\n",
        "#     result = minimize(f, initial_point, method='BFGS', jac=partial(numeric_gradient, f))\n",
        "#     print(f\"Результат {method} на функции {name}:\")\n",
        "#     print(\"Минимум:\", result.x)\n",
        "#     print(np.linalg.norm(result.x - min_))\n",
        "#     print(\"Количество итераций:\", result.nit)\n",
        "#     print(\"Количество вычислений функции:\", result.nfev)\n",
        "#     print(\"Количество вычислений градиента:\", result.njev)\n",
        "#     print()\n",
        "#   print(\"_________________\")\n",
        "#   print()"
      ],
      "metadata": {
        "id": "wCUTKspa6aoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = list(test_functions.items())[1]\n",
        "print(f)\n",
        "# print(nd.Hessian(f[1])(np.array([1.0, 2.0])))\n",
        "ans = newton_cg(f[1], np.array([0, 0]), constant_scheduler)\n",
        "print(ans[0], len(ans[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlRb961JM-9P",
        "outputId": "5ee664ed-3272-4ffb-a5cb-75d6d2c59603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Quadratic_2', <function <lambda> at 0x795f7ea25080>)\n",
            "[3. 3.] 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "# Тестовая секция\n",
        "\n",
        "### Тест функции `Quadratic_1` : $x ^ 2 + y ^ 2 - 6x - 8 y + 10$\n",
        "Метод | Старт | Количество итераций | Расстояние до минимума | Ближайшая точка минимума\n",
        ":-|:-:|:-:|:-:|:-:\n",
        "`Constant scheduler`| `(-0.96, -0.167)` | 801 | 4.97e-7 | `(3, 4)`\n",
        "`Exp scheduler`| `(-0.96, -0.167)` | 306 | 4.73e-7 | `(3, 4)`\n",
        "`Polynom scheduler`| `(-0.96, -0.167)` | 306 | 4.73e-7 | `(3, 4)`\n",
        "`Partial const scheduler`| `(-0.96, -0.167)` | 7280 | 5.1e-3 | `(3, 4)`\n",
        "`Armijo`| `(-0.96, -0.167)` | 14 | 3.34e-7 | `(3, 4)`\n",
        "`Wolfe`| `(-0.96, -0.167)` | 33 | 4.49e-7 | `(3, 4)`\n",
        "`Dihotomy`| `(-0.96, -0.167)` | 2 | 6.51e-8 | `(3, 4)`\n",
        "`Golden ratio`| `(-0.96, -0.167)` | 3 | 4.27e-8 | `(3, 4)`\n",
        "`BFGS `| `(-0.96, -0.167)` | 3 | 1.09e-7 | `(3, 4)`\n",
        "`CG `| `(-0.96, -0.167)` | 3 | 1.09e-7 | `(3, 4)`\n",
        "\n",
        "<br>\n",
        "\n",
        "-------\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Тест функции `Quadratic_2`: $2 x ^ 2 + 3 y ^ 2 - 12 x -18y + 20$\n",
        "\n",
        "\n",
        "Метод | Старт | Количество итераций | Расстояние до минимума | Ближайшая точка минимума\n",
        ":-|:-:|:-:|:-:|:-:\n",
        "`Constant scheduler`| `(1.16, -0.32)` | 408 | 2.42e-7 | `(3, 3)`\n",
        "`Exp scheduler`| `(1.16, -0.32)` | 141 | 2.38e-7  | `(3, 3)`\n",
        "`Polynom scheduler`| `(1.16, -0.32)` | 71 | 1.37e-5 | `(3, 3)`\n",
        "`Partial const scheduler`| `(1.16, -0.32)` | 3226 | 8.49e-5 | `(3, 3)`\n",
        "`Armijo`| `(1.16, -0.32)` | 14 | 8.76e-8 | `(3, 3)`\n",
        "`Wolfe`| `(1.16, -0.32)`  | 33 | 9.07e-8 | `(3, 3)`\n",
        "`Dihotomy`| `(1.16, -0.32)` | 12 | 6.16e-8 | `(3, 4)`\n",
        "`Golden ratio`| `(1.16, -0.32)` | 12 | 6.22e-8 | `(3, 3)`\n",
        "`BFGS `| `(1.16, -0.32)` | 5 | 8.14e-09 | `(3, 3)`\n",
        "`CG `| `(1.16, -0.32)` |5 | 8.14e-09 | `(3, 3)`\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "-------\n",
        "\n",
        "<br>\n",
        "\n",
        "### Тест функции `Quadratic_3`: $4 x ^ 2 + 5 y ^ 2 + 2xy -16 x - 20 y + 15$\n",
        "Метод | Старт | Количество итераций | Расстояние до минимума | Ближайшая точка минимума\n",
        ":-|:-:|:-:|:-:|:-:\n",
        "`Constant scheduler`| `(0.53, -0.72)` | 211 | 1.55e-7  | `(30/19, 32/19)`\n",
        "`Exp scheduler`|`(0.53, -0.72)` | 69| 1.32e-7  | `(30/19, 32/19)`\n",
        "`Polynom scheduler`| `(0.53, -0.72)` | 37 | 1.28e-7 | `(30/19, 32/19)`\n",
        "`Partial const scheduler`| `(0.53, -0.72)` | 1476 | 4.76e-7 | `(30/19, 32/19)`\n",
        "`Armijo`| `(0.53, -0.72)`| 13  | 5.06e-8 | `(30/19, 32/19)`\n",
        "`Wolfe`|  `(0.53, -0.72)`|137 | 7.58e-8 | `(30/19, 32/19)`\n",
        "`Dihotomy`| `(0.53, -0.72)`| 7 | 5.08e-8 | `(30/19, 32/19)`\n",
        "`Golden ratio`| `(0.53, -0.72)`| 7 | 2.88e-8 | `(30/19, 32/19)`\n",
        "`BFGS `| `(0.53, -0.72)` | 5 | 1.17e-08 | `(30/19, 32/19)`\n",
        "`CG `| `(0.53, -0.72)` | 5 | 1.17e-08 | `(30/19, 32/19)`\n",
        "\n",
        "<br>\n",
        "\n",
        "-------\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Тест функции `Multimodal_trigonom`: $\\sin(0.5 \\cdot x) ^ 2 + \\sin(0.5 \\cdot y) ^ 2$\n",
        "Метод | Старт | Количество итераций | Расстояние до минимума | Ближайшая точка минимума\n",
        ":-|:-:|:-:|:-:|:-:\n",
        "`Constant scheduler`| `(0.22, -0.52)` | 2513 | 1.99e-6 | `(0, 0)`\n",
        "`Exp scheduler`| `(0.22, -0.52)` | 1805 | 1.99e-6 | `(0, 0)`\n",
        "`Polynom scheduler`| `(0.22, -0.52)` | 540 | 1.22e-6 | `(0, 0)`\n",
        "`Partial const scheduler`| `(0.22, -0.52)` | 26307 | 1.22e-6 | `(0, 0)`\n",
        "`Armijo`| `(0.22, -0.52)` | 22  | 1.17e-6 | `(0, 0)`\n",
        "`Wolfe`| `(0.22, -0.52)` | 21 | 1.57e-6 | `(0, 0)`\n",
        "`Dihotomy`| `(0.22, -0.52)` | 23 |1.17e-6  | `(0, 0)`\n",
        "`Golden ratio`| `(0.22, -0.52)` | 20 | 1.57e-6 | `(0, 0)`\n",
        "`BFGS `| `(0.22, -0.52)` | 4 | 1.96e-5 | `(0, 0)`\n",
        "`CG `| `(0.22, -0.52)` |4 | 1.96e-5 | `(0, 0)`\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "-------\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Тест функции `Hard_function_1`: $-\\frac 1 {x ^ 2 + y ^ 2 + 1} - \\frac 1 {((x-2) ^ 2 + (y - 2)  ^ 2  + 1)^2}$\n",
        "Метод | Старт | Количество итераций | Расстояние до минимума | Ближайшая точка минимума\n",
        ":-|:-:|:-:|:-:|:-:\n",
        "`Constant scheduler`| `(-1.38, -0.28)` |881 | 0.007 | `(0, 0)`\n",
        "`Exp scheduler`|  `(-1.38, -0.28)`| 347 | 0.006 | `(0, 0)`\n",
        "`Polynom scheduler`|  `(-1.38, -0.28)`| 228 | 0.005 | `(0, 0)`\n",
        "`Partial const scheduler`|  `(-1.38, -0.28)`| 10914 | 0.005 | `(0, 0)`\n",
        "`Armijo`| `(-1.38, -0.28)` | 12 | 0.0078 | `(0, 0)`\n",
        "`Wolfe`|  `(-1.38, -0.28)` | 23 | 0.0075 | `(0, 0)`\n",
        "`Dihotomy`| `(-1.38, -0.28)` | 9 | 0.005 | `(0, 0)`\n",
        "`Golden ratio`| `(-1.38, -0.28)` | 8 | 0.0056 | `(0, 0)`\n",
        "`BFGS `| `(-1.38, -0.28)` | 4 | 0.007 | `(0, 0)`\n",
        "`CG `| `(-1.38, -0.28)` |4 | 0.007 | `(0, 0)`\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "-------\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### Тест функции `Himmelblau`: $(x ^ 2 + y - 11) ^ 2 + (x + y ^ 2 - 7) ^ 2$\n",
        "Метод | Старт | Количество итераций | Расстояние до минимума | Ближайшая точка минимума\n",
        ":-|:-:|:-:|:-:|:-:\n",
        "`Constant scheduler`| `(-0.0031, 0.206)` | 64 | 3.88e-8| `(3, 2)`\n",
        "`Exp scheduler`| `(-0.0031, 0.206)` |337 | 1.08e-8 | `(3, 2)`\n",
        "`Polynom scheduler`| `(-0.0031, 0.206)` | - | - | `(3, 2)`\n",
        "`Partial const scheduler`| `(-0.0031, 0.206)` | - | - | `(3, 2)`\n",
        "`Armijo`| `(-0.0031, 0.206)` | 30 | 1.43e-8 | `(3, 2)`\n",
        "`Wolfe`| `(2.039, -0.004)` | 117 | 1.159e-8 | `(3, 2)`\n",
        "`Dihotomy`| `(-0.0031, 0.206)` | 17 | 2.01e-8 | `(3, 2)`\n",
        "`Golden ratio`| `(-0.0031, 0.206)` | 16 | 2.69e-8 | `(3, 2)`\n",
        "`BFGS `| `(-0.0031, 0.206)` | 10 | 7.08e-10 | `(3, 2)`\n",
        "`CG `| `(-0.0031, 0.206)` | 12 | 7.08e-10 | `(3, 2)`\n",
        "\n"
      ],
      "metadata": {
        "id": "v1UeTG5tvxoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center></center>\n",
        "### **<center>Заключение</center>**\n",
        "\n",
        "Как и ожидалось, хуже всего себя показал `Constant sheduler` (особенно на сложных функциях, порядка 2500 итераций в худшем случае). Немногим лучше работал выбор шага с экпоненциальным затуханием - 1800 итераций для функции `Multimodal_trigonom`. `Polynomial sheduler` и `Partial const sheduler`не нашли экстремум функции `Himmelblau`. Правило Вольфе-Пауэла в среднем работало за 30 итераций, но на функциях `Himmelblau` и `Quadratic_3` число итераций превышало 100. Стоит отметить, что на функциях с большим количеством точек перегиба правила Арамихо и Вольфе-Пауэлла могли работать как за 15-20 итераций (в лучшем случае), так и за >100 итераций. Самыми стабильными оказались метод золотого сечения и метод дихотомии."
      ],
      "metadata": {
        "id": "pYFXxLjqWXup"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}